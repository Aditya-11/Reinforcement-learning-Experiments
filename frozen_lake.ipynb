{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frozen-lake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOW4ThhmkOq/TPF/TL2G5b1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-11/Reinforcement-learning-Experiments/blob/master/frozen_lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRO4z__0JDw9",
        "colab_type": "code",
        "outputId": "2fa165dc-faf6-48d5-d0a1-db4b0b43797d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# used by the OpenAI environment to train the agent\n",
        "# Made by Aditya Dubey\n",
        "\n",
        "import numpy as np\n",
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EocZKEhp8w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agwIXHgSqDXB",
        "colab_type": "text"
      },
      "source": [
        "Frozen lake implementation by using Q learning , agent has to travel from start to end and reach the end destination.\n",
        "\n",
        "The map of the game looks something like this , agent has to start from s and can end the game either reaching goal or falling in the hole. Agent can only walk on the frozen paths\n",
        "\n",
        "![alt text](https://analyticsindiamag.com/wp-content/uploads/2018/03/Frozen-Lake.png)   ![alt text](https://analyticsindiamag.com/wp-content/uploads/2018/03/description.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2DbzJGvwXP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using open AI gym implementation\n",
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whTZNBs6wkKX",
        "colab_type": "code",
        "outputId": "5398843b-13f0-46d3-9fb6-43016a304cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "# now we will calculate action and state size of the environment\n",
        "\n",
        "action_size = env.action_space.n #columns\n",
        "state_size = env.observation_space.n # rows\n",
        "\n",
        "q_table = np.zeros((state_size,action_size))\n",
        "\n",
        "print(q_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGAZ3rIxYHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create hyper parameters for the learning algorithm \n",
        "\n",
        "total_episodes = 25000        # Total episodes\n",
        "learning_rate = 0.9           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.0055            # Exponential decay rate for exploration prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyHGevUnyEd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzgnWgDZyS8W",
        "colab_type": "text"
      },
      "source": [
        "# Q learning algrithm to be implemented\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/1ee37cfc3130057f828f19b3cee6066d41c1eeb4/Q%20learning/FrozenLake/qtable_algo.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-u3RREvypVA",
        "colab_type": "code",
        "outputId": "abb7a5ea-f054-45fe-ac77-61b106eb191f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# Q learning algorithm to be implemented , Bellman equation to update the Q table value\n",
        "\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "  # reset the env\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  total_rewards = 0\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    # choose an action a in the current world\n",
        "\n",
        "    e_tradeoff = random.uniform(0,1)\n",
        "\n",
        "    \"\"\"\n",
        "    if e_tradeoff > epsilon :\n",
        "      Exploitation mode -> Use Q table to choose the action \n",
        "    else : \n",
        "      Exploration mode -> choose a random action \n",
        "    \"\"\"\n",
        "\n",
        "    if (e_tradeoff > epsilon) :\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    else :\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    # take action a and go to state s' and reward r\n",
        "\n",
        "    new_state,reward,done,info = env.step(action)\n",
        "\n",
        "    # implementing the belman equation \n",
        "    q_table[state,action] = q_table[state,action] +  learning_rate * (reward + gamma * np.max(q_table[new_state,:]) - q_table[state,action])\n",
        "\n",
        "    total_rewards += reward\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "    # if we finish the episode\n",
        "\n",
        "    if done == True : \n",
        "      break\n",
        "\n",
        "  \n",
        "  # update the epsilon value (exploration vs exploitation value)\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "  rewards.append(total_rewards)\n",
        "\n",
        "\n",
        "print(\"Score over the time : \",(sum(rewards)/int(total_episodes)))\n",
        "\n",
        "print(q_table)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over the time : % 0.4570625\n",
            "[[1.50140905e-01 9.00963537e-03 1.13636676e-02 1.50717827e-02]\n",
            " [7.84611517e-04 2.86960773e-03 5.85236242e-03 1.44002062e-01]\n",
            " [2.87594883e-03 3.85480153e-03 3.59224704e-02 5.35481182e-03]\n",
            " [3.76257363e-03 5.23720359e-03 1.63803104e-04 9.16885860e-03]\n",
            " [2.95854800e-01 2.67329729e-02 1.62931593e-02 3.17779411e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.68108893e-06 5.73440649e-08 6.09739135e-03 1.28955435e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.80813885e-04 5.62046869e-03 2.05348039e-03 3.78612361e-01]\n",
            " [6.37456429e-04 1.29850151e-01 2.23640710e-03 5.59741789e-03]\n",
            " [4.97134720e-01 3.32246978e-03 1.08175324e-03 1.93042151e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.88701509e-02 1.97962628e-01 3.11010958e-01 8.05291834e-02]\n",
            " [1.20238148e-01 9.56906356e-01 1.14637739e-01 9.94023805e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNca5PUkALrB",
        "colab_type": "code",
        "outputId": "37eac10a-fd03-4e8d-9079-66b1e252be61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(10):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  print(\"EPISODE % *****************************************************\",episode)\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    new_state , reward , done , info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      # print the last state whether the agent has reahed the goal or pithole\n",
        "      env.render() \n",
        "      print(\"Number of Steps \",step)\n",
        "      break\n",
        "  state = new_state\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE % ***************************************************** 0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 24\n",
            "EPISODE % ***************************************************** 1\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 11\n",
            "EPISODE % ***************************************************** 2\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 23\n",
            "EPISODE % ***************************************************** 3\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 6\n",
            "EPISODE % ***************************************************** 4\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 10\n",
            "EPISODE % ***************************************************** 5\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 15\n",
            "EPISODE % ***************************************************** 6\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 3\n",
            "EPISODE % ***************************************************** 7\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 8\n",
            "EPISODE % ***************************************************** 8\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 5\n",
            "EPISODE % ***************************************************** 9\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crHGbibN_IGu",
        "colab_type": "code",
        "outputId": "773361aa-1dfd-4b38-b28f-4adac3af0105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# Q learning algorithm to be implemented , Bellman equation to update the Q table value\n",
        "# training the Q table differently , lets not choose exploration / exploitation randomly and keep a threshold\n",
        "\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "  # reset the env\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  total_rewards = 0\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    # choose an action a in the current world\n",
        "\n",
        "    #e_tradeoff = random.uniform(0,1)\n",
        "\n",
        "    \"\"\"\n",
        "    if e_tradeoff > epsilon :\n",
        "      Exploitation mode -> Use Q table to choose the action \n",
        "    else : \n",
        "      Exploration mode -> choose a random action \n",
        "    \"\"\"\n",
        "\n",
        "    if (epsilon>0.59) :\n",
        "      action = np.argmax(q_table[state,:])\n",
        "    else :\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    # take action a and go to state s' and reward r\n",
        "\n",
        "    new_state,reward,done,info = env.step(action)\n",
        "\n",
        "    # implementing the belman equation \n",
        "    q_table[state,action] = q_table[state,action] +  learning_rate * (reward + gamma * np.max(q_table[new_state,:]) - q_table[state,action])\n",
        "\n",
        "    total_rewards += reward\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "    # if we finish the episode\n",
        "\n",
        "    if done == True : \n",
        "      break\n",
        "\n",
        "  \n",
        "  # update the epsilon value (exploration vs exploitation value)\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "  rewards.append(total_rewards)\n",
        "\n",
        "\n",
        "print(\"Score over the time : \",(sum(rewards)/int(total_episodes)))\n",
        "\n",
        "print(q_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over the time : % 0.0158125\n",
            "[[6.63215779e-01 7.18280218e-01 7.30380770e-01 7.17983046e-01]\n",
            " [6.83328699e-01 6.76742088e-01 7.59177790e-01 7.52128087e-01]\n",
            " [7.21332006e-01 7.66943613e-01 7.63085280e-01 7.53879717e-01]\n",
            " [7.41052946e-04 6.56181904e-01 6.04659262e-01 5.89370803e-01]\n",
            " [7.70167545e-01 6.93771102e-01 6.91297178e-01 6.29056998e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.40560754e-01 8.51010639e-03 8.07873274e-01 6.46948598e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.23186065e-02 8.11427307e-01 7.14526513e-01 7.23431662e-01]\n",
            " [8.09720572e-01 7.79233834e-01 8.14918756e-01 8.58738695e-01]\n",
            " [8.13263780e-01 8.58158053e-01 7.64397824e-01 7.41585770e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.15463166e-01 8.15561687e-01 8.57785965e-01 7.35071860e-02]\n",
            " [8.62910956e-01 8.14377401e-01 8.24433588e-01 8.55405452e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itQwi36S7T1w",
        "colab_type": "code",
        "outputId": "28cd0bf9-9f69-4141-b853-91bac107d2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Lets play the game using the Q_table which we trained\n",
        "\n",
        "env.reset()\n",
        "\n",
        "for episode in range(10):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  print(\"EPISODE % *****************************************************\",episode)\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    new_state , reward , done , info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      # print the last state whether the agent has reahed the goal or pithole\n",
        "      env.render() \n",
        "      print(\"Number of Steps %\",step)\n",
        "      break\n",
        "  state = new_state\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE % ***************************************************** 0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 2\n",
            "EPISODE % ***************************************************** 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Number of Steps % 2\n",
            "EPISODE % ***************************************************** 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 4\n",
            "EPISODE % ***************************************************** 3\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of Steps % 11\n",
            "EPISODE % ***************************************************** 4\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 8\n",
            "EPISODE % ***************************************************** 5\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 6\n",
            "EPISODE % ***************************************************** 6\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 3\n",
            "EPISODE % ***************************************************** 7\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 8\n",
            "EPISODE % ***************************************************** 8\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 3\n",
            "EPISODE % ***************************************************** 9\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Steps % 7\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}