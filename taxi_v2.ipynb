{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taxi-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcCaDzo+y7NY5djxkROzvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-11/Reinforcement-learning-Experiments/blob/master/taxi_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abuGLA33-AF2",
        "colab_type": "code",
        "outputId": "0db20fb6-de9d-4f51-c791-9e3bea6c8fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNWz6Mk2-V3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMT7Go3Oi0re",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The agents (taxi) task will to pick up the passenger and drop him to the destination with least number of timesteps.\n",
        "\n",
        "There are 6 discrete deterministic actions:\n",
        "* 0: move down\n",
        "* 1: move up\n",
        "* 2: move to the right\n",
        "* 3: move to the left\n",
        "* 4: pick up a passenger\n",
        "* 5: drop-off a passenger\n",
        "\n",
        "The color coding is as follows:\n",
        "* blue: passenger\n",
        "* magenta: destination\n",
        "* yellow: empty taxi\n",
        "* green: full taxi\n",
        "* other letters: locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9cvjnJmJf4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "d8e2da53-81f1-47ae-fc99-6f5259a9a83f"
      },
      "source": [
        "env = gym.make('Taxi-v3').env\n",
        "\n",
        "env.reset()\n",
        "\n",
        "env.render()\n",
        "\n",
        "print('Action Space ',env.action_space.n)\n",
        "\n",
        "print('State Space ',env.observation_space.n)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "\n",
            "Action Space  6\n",
            "State Space  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_T5rI3xGow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c33f7ce0-89ca-4cd5-e403-0b7ac6577e69"
      },
      "source": [
        "# check out position \n",
        "\n",
        "state = env.encode(3,1,0,2)\n",
        "\n",
        "print('state is : ',state)\n",
        "\n",
        "env.s = state\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state is :  322\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QOIBX5Z3YzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5022f800-68a9-4ec4-d88b-1a3ca707943d"
      },
      "source": [
        "# see the reward table for a state P -> state * action \n",
        "env.P[328]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5QiXBtn43kH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "597123ff-b313-4e21-c389-b6263db50140"
      },
      "source": [
        "# solving the env without reinforcement learning\n",
        "\n",
        "epochs = 0\n",
        "penalties = 0\n",
        "reward = 0\n",
        "\n",
        "frames = [] # for animation \n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  action = env.action_space.sample()\n",
        "  state ,reward, done, info = env.step(action)\n",
        "\n",
        "  if reward == -10:\n",
        "    penalties += 1\n",
        "\n",
        "  # put each frame into dict\n",
        "\n",
        "  frames.append(\n",
        "      {\n",
        "          'frame' : env.render(mode = 'ansi'),\n",
        "          'state' : state,\n",
        "          'action' : action,\n",
        "          'reward' : reward\n",
        "      }\n",
        "  )\n",
        "\n",
        "  epochs += 1\n",
        "\n",
        "print ('total time steps taken : ',epochs)\n",
        "print ('penalties incurred : ' , penalties)\n",
        "\n",
        "# render the frames\n",
        "\n",
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "\n",
        "for i in range(len(frames)):\n",
        "  clear_output(wait = True)\n",
        "  print ('Timestep : ',i+1)\n",
        "  print (frames[i]['frame'])\n",
        "  print ('state -> ',frames[i]['state'])\n",
        "  print ('action -> ',frames[i]['action'])\n",
        "  print ('reward -> ',frames[i]['reward'])\n",
        "  sleep(0.1)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timestep :  211\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "state ->  410\n",
            "action ->  5\n",
            "reward ->  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWLIZNU_Ugza",
        "colab_type": "text"
      },
      "source": [
        "bellman equation to update the value\n",
        "\n",
        "\n",
        "Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1acsPzK_8_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "5505da00-8462-4954-bd1b-5747845c37be"
      },
      "source": [
        "# Using Q learning and Bellman equation to train the agent\n",
        "\n",
        "import math\n",
        "\n",
        "q_table = np.zeros((env.observation_space.n,env.action_space.n,))\n",
        "\n",
        "# no of episodes to be trained\n",
        "\n",
        "no_episodes = 135000\n",
        "\n",
        "# hyper parameters\n",
        "\n",
        "learning_rate = 0.88\n",
        "discount_rate = 0.93\n",
        "#decay_rate = 0.0001\n",
        "\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.000001\n",
        "min_epsilon = 0.00005\n",
        "\n",
        "epochs = 0\n",
        "penalties = 0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "for i in range(no_episodes):\n",
        "  state = env.reset()\n",
        "\n",
        "  penalties , reward = 0,0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    if random.uniform(0,1) < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = np.argmax(q_table[state , :])\n",
        "\n",
        "    next_state , reward , done , info = env.step(action)\n",
        "\n",
        "    if (reward == -10):\n",
        "      penalties += 1\n",
        "\n",
        "    next_max = np.max(q_table[next_state])\n",
        "\n",
        "    q_table[state,action] = (1 - learning_rate) * q_table[state,action] + learning_rate * (reward + discount_rate * next_max)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "    if (epsilon > min_epsilon):\n",
        "      epsilon = epsilon -  epsilon_decay\n",
        "    else:\n",
        "      epsilon = epsilon \n",
        "  \n",
        "  if (i%100==0):\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Episode: {i}\")\n",
        "    print(\"epsilom : \", epsilon)\n",
        "    #print(env.render())\n",
        "\n",
        "\n",
        "print(env.render())\n",
        "\n",
        "print(\"Average Epochs per episode : {}\".format(epochs/no_episodes))\n",
        "\n",
        "print(\"Average Penalties per episode : {}\".format(penalties/no_episodes))\n",
        "\n",
        "\n",
        "\n",
        "print(q_table)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 134900\n",
            "epsilom :  4.999999208169932e-05\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "None\n",
            "Average Epochs per episode : 17.036585185185185\n",
            "Average Penalties per episode : 0.0\n",
            "[[ 0.          0.          0.          0.          0.          0.        ]\n",
            " [ 1.14640728  2.30796482  1.14640728  2.30796482  3.55695142 -6.69203518]\n",
            " [ 6.34402985  7.89680629  6.34402985  7.89680629  9.56645838 -1.10319371]\n",
            " ...\n",
            " [ 9.56645838 11.3617832   9.56645838  7.89680629  0.56645838  0.56645838]\n",
            " [ 3.55695142  4.89994776  3.55695142  4.89994776 -5.44304858 -5.44304858]\n",
            " [15.368      13.29224    15.368      17.6         6.368       6.368     ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKgw0QK9I01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a81990ec-3434-48a0-d9c4-f77376b36799"
      },
      "source": [
        "# testing the agent\n",
        "\n",
        "total_epochs , total_penalties = 0,0\n",
        "episode = 100\n",
        "\n",
        "\n",
        "for i in range(episode):\n",
        "  state = env.reset()\n",
        "  penalties , reward = 0, 0 \n",
        "  done = False\n",
        "\n",
        "  while not done: \n",
        "    action = np.argmax(q_table[state,:])\n",
        "    next_state , reward , done , info = env.step(action)\n",
        "    if (reward == -10):\n",
        "      total_penalties =+ 1\n",
        "    #epochs =+ 1\n",
        "    state = next_state\n",
        "    total_epochs += 1\n",
        "\n",
        "print(\"Average epochs per episode : {}\".format(total_epochs/episode))\n",
        "\n",
        "print(\"Average penalties per episode : {}\".format(total_penalties/episode))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average epochs per episode : 13.11\n",
            "Average penalties per episode : 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FtZQj7ff9ZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6ebbd6f1-4e44-4e38-dedf-31ab5fa450ad"
      },
      "source": [
        "\n",
        "# programme illustrates the working of agent\n",
        "\n",
        "\n",
        "episode = 1\n",
        "\n",
        "total_epochs = 0\n",
        "\n",
        "for i in range(episode):\n",
        "  state = env.reset()\n",
        "  penalties , reward = 0, 0 \n",
        "  done = False\n",
        "\n",
        "  while not done: \n",
        "    action = np.argmax(q_table[state,:])\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    sleep(1.1)\n",
        "    next_state , reward , done , info = env.step(action)\n",
        "    state = next_state\n",
        "    total_epochs += 1\n",
        "  print(\"total_epochs :\",total_epochs)\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "total_epochs : 12\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}